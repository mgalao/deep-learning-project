{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: darkslategray; color: white; padding: 15px; border-radius: 8px;\">\n",
    "    <center><h1 style=\"font-family: Arial, sans-serif;\">Predicting Rare Species</h1></center>\n",
    "    <center><h3 style=\"font-family: Arial, sans-serif;\">Deep Learning Project</h3></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Table of Contents</h3>**\n",
    "* [1. Environment Setup](#1-environment-setup)\n",
    "    * [1.1 Import Libraries](#11-import-libraries)\n",
    "    * [1.2 Import Dataset](#12-import-dataset)\n",
    "* [2. Exploratory Data Analysis](#2-exploratory-data-analysis)\n",
    "    * [2.1 Visualizations](#21-visualizations)\n",
    "    * [2.2 Import Dataset](#22-feature-engineering)\n",
    "* [3. Splitting the Dataset](#3-splitting-the-dataset)\n",
    "* [4. Splitting the Dataset](#4-preprocessing)\n",
    "    * [4.1 Image Generator and Augmentation](#41-image-generator-and-augmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **1.** Environment Setup\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Connect Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Change to the directory where project is located\n",
    "# os.chdir('/content/drive/MyDrive/College/MSc/2nd Semester/Deep Learning/project')\n",
    "\n",
    "# # Verify that we changed the directory\n",
    "# print(\"Changed directory to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Google Colab\n",
    "# !pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\carolina\\documents\\mestrado\\deep_learning\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from classes import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Concatenate, Dropout, Input, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from keras.metrics import AUC, F1Score, CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of data we have is not supported by GitHub (where we have our project stored). The solution is: create a folder named data and allocate the rare_species file inside it. The gitignore file makes sure this folder is not used when we are pulling or pushing changes but everyone needs to have it on their machines locally. A random seed was used to ensure that the splits stay the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google Colab\n",
    "\n",
    "# # Define the path to the zip file and the extraction folder\n",
    "# zip_path = Path(\"../data/rare_species.zip\")\n",
    "\n",
    "# # Get the directory where the zip file is located\n",
    "# extract_path = Path(\"../data/rare_species\")\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rare_species_id</th>\n",
       "      <th>eol_content_id</th>\n",
       "      <th>eol_page_id</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75fd91cb-2881-41cd-88e6-de451e8b60e2</td>\n",
       "      <td>12853737</td>\n",
       "      <td>449393</td>\n",
       "      <td>animalia</td>\n",
       "      <td>mollusca</td>\n",
       "      <td>unionidae</td>\n",
       "      <td>mollusca_unionidae/12853737_449393_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28c508bc-63ff-4e60-9c8f-1934367e1528</td>\n",
       "      <td>20969394</td>\n",
       "      <td>793083</td>\n",
       "      <td>animalia</td>\n",
       "      <td>chordata</td>\n",
       "      <td>geoemydidae</td>\n",
       "      <td>chordata_geoemydidae/20969394_793083_eol-full-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00372441-588c-4af8-9665-29bee20822c0</td>\n",
       "      <td>28895411</td>\n",
       "      <td>319982</td>\n",
       "      <td>animalia</td>\n",
       "      <td>chordata</td>\n",
       "      <td>cryptobranchidae</td>\n",
       "      <td>chordata_cryptobranchidae/28895411_319982_eol-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29cc6040-6af2-49ee-86ec-ab7d89793828</td>\n",
       "      <td>29658536</td>\n",
       "      <td>45510188</td>\n",
       "      <td>animalia</td>\n",
       "      <td>chordata</td>\n",
       "      <td>turdidae</td>\n",
       "      <td>chordata_turdidae/29658536_45510188_eol-full-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94004bff-3a33-4758-8125-bf72e6e57eab</td>\n",
       "      <td>21252576</td>\n",
       "      <td>7250886</td>\n",
       "      <td>animalia</td>\n",
       "      <td>chordata</td>\n",
       "      <td>indriidae</td>\n",
       "      <td>chordata_indriidae/21252576_7250886_eol-full-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        rare_species_id  eol_content_id  eol_page_id  \\\n",
       "0  75fd91cb-2881-41cd-88e6-de451e8b60e2        12853737       449393   \n",
       "1  28c508bc-63ff-4e60-9c8f-1934367e1528        20969394       793083   \n",
       "2  00372441-588c-4af8-9665-29bee20822c0        28895411       319982   \n",
       "3  29cc6040-6af2-49ee-86ec-ab7d89793828        29658536     45510188   \n",
       "4  94004bff-3a33-4758-8125-bf72e6e57eab        21252576      7250886   \n",
       "\n",
       "    kingdom    phylum            family  \\\n",
       "0  animalia  mollusca         unionidae   \n",
       "1  animalia  chordata       geoemydidae   \n",
       "2  animalia  chordata  cryptobranchidae   \n",
       "3  animalia  chordata          turdidae   \n",
       "4  animalia  chordata         indriidae   \n",
       "\n",
       "                                           file_path  \n",
       "0  mollusca_unionidae/12853737_449393_eol-full-si...  \n",
       "1  chordata_geoemydidae/20969394_793083_eol-full-...  \n",
       "2  chordata_cryptobranchidae/28895411_319982_eol-...  \n",
       "3  chordata_turdidae/29658536_45510188_eol-full-s...  \n",
       "4  chordata_indriidae/21252576_7250886_eol-full-s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Metadata\n",
    "metadata_path = Path(\"../data/rare_species/metadata.csv\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11983, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # 11983 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **2.** Exploratory Data Analysis !!!Not Needed\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kingdom_counts = df['kingdom'].value_counts()\n",
    "# plot_graph(title='Distribution of Observations Across Kingdom', xlabel='Kingdom', ylabel='Number of Observations',  counts=kingdom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phylum_counts = df['phylum'].value_counts()\n",
    "# plot_graph(title='Distribution of Observations Across Phylum', xlabel='Phylum', ylabel='Number of Observations',  counts=phylum_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_counts = df['family'].value_counts()\n",
    "# plot_graph(title='Distribution of Observations Across Family', xlabel='Family', ylabel='Number of Observations',  counts=family_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_counts = df[\"family\"].value_counts(normalize=True)*100\n",
    "\n",
    "# df_family_proportions = pd.DataFrame({\n",
    "#     \"Family\": family_counts.index,\n",
    "#     \"Proportion (%)\": family_counts.values  \n",
    "# })\n",
    "\n",
    "# df_family_proportions # 202 different families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_families_025 = int((df_family_proportions[\"Proportion (%)\"] <= 0.2505).sum())\n",
    "# num_families_025 # 118 families have less than 0.2505% of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_100_families = df['family'].value_counts().nlargest(100)\n",
    "# plot_graph(title='Distribution of Observations Across Top 100 Families', xlabel='Family', ylabel='Number of Observations',  counts=top_100_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_84_family_proportion_sum = df_family_proportions.iloc[:84][\"Proportion (%)\"].sum()\n",
    "# top_84_family_proportion_sum # although 118 families represent less than 0.25% of the dataset, the 84 that have >0.25% represent only 70.46% of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- is it better to predict phylum and then family or a concatenation of the two? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"phylum_family\"] = df[\"phylum\"] + \"_\" + df[\"family\"]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_phylum_counts = df.groupby(\"phylum\")[\"family\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_phylum_counts # the phylum with the most families is chordata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_counts = df.groupby(\"family\")[\"phylum\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_counts.value_counts() # there are no families that belong to more than 1 phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phylum_family_counts = df[\"phylum_family\"].value_counts(normalize=True)*100\n",
    "\n",
    "# df_phylum_family_proportions = pd.DataFrame({\n",
    "#     \"phylum_family\": phylum_family_counts.index,\n",
    "#     \"Proportion (%)\": phylum_family_counts.values.round(4)  \n",
    "# })\n",
    "\n",
    "# df_phylum_family_proportions # 202 different families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **3.** Feature Engineering !!!Not Needed\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique phyla: 5\n",
      "Number of unique families: 202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Encode family (target) and phylum (metadata feature)\n",
    "family_encoder = LabelEncoder()\n",
    "phylum_encoder = LabelEncoder()\n",
    "df[\"family_encoded\"] = family_encoder.fit_transform(df[\"family\"])\n",
    "df[\"phylum_encoded\"] = phylum_encoder.fit_transform(df[\"phylum\"])\n",
    "\n",
    "# Check the number of unique families and phyla\n",
    "num_families = df[\"family_encoded\"].nunique()\n",
    "num_phyla = df[\"phylum_encoded\"].nunique()\n",
    "print(f\"Number of unique phyla: {num_phyla}\")\n",
    "print(f\"Number of unique families: {num_families}\")\n",
    "\n",
    "# One-hot encode the family and phylum columns using NumPy (not eager tensors)\n",
    "df[\"phylum_onehot\"] = df[\"phylum_encoded\"].apply(lambda x: tf.one_hot(x, depth=num_phyla).numpy())\n",
    "df[\"family_onehot\"] = df[\"family_encoded\"].apply(lambda x: tf.one_hot(x, depth=num_families).numpy())\n",
    "\n",
    "# Create a new column for the full file path\n",
    "# Make it OS-independent using Path\n",
    "df[\"full_file_path\"] = df[\"file_path\"].apply(lambda x: str(Path(\"../data/rare_species\") / x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode family (target) and phylum (metadata feature)\n",
    "# family_encoder = LabelEncoder()\n",
    "# phylum_encoder = LabelEncoder()\n",
    "# df[\"family_encoded\"] = family_encoder.fit_transform(df[\"family\"])\n",
    "# df[\"phylum_encoded\"] = phylum_encoder.fit_transform(df[\"phylum\"])\n",
    "\n",
    "# # Check the number of unique families and phyla\n",
    "# num_families = df[\"family_encoded\"].nunique()\n",
    "# num_phyla = df[\"phylum_encoded\"].nunique()\n",
    "# print(f\"Number of unique phyla: {num_phyla}\")\n",
    "# print(f\"Number of unique families: {num_families}\")\n",
    "\n",
    "# # One-hot encode the family and phylum columns\n",
    "# df[\"phylum_onehot\"] = df[\"phylum_encoded\"].apply(lambda x: tf.one_hot(x, depth=num_phyla).numpy())\n",
    "# df[\"family_onehot\"] = df[\"family_encoded\"].apply(lambda x: tf.one_hot(x, depth=num_families).numpy())\n",
    "\n",
    "# # Create a new column for the full file path\n",
    "# # Windows\n",
    "# df[\"full_file_path\"] = df[\"file_path\"].apply(lambda x: os.path.join(path, x.replace(\"/\", \"\\\\\")))\n",
    "# # Mac\n",
    "# df[\"full_file_path\"] = df[\"file_path\"].apply(lambda x: str(Path(\"../data/rare_species\") / x))\n",
    "\n",
    "# # Increase display width\n",
    "# # pd.set_option('display.max_colwidth', None)\n",
    "# # pd.reset_option('display.max_colwidth')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **4.** Splitting the Dataset !!!Not Needed\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "train_df, test_valid_df = train_test_split(df, test_size=0.3, shuffle=True, random_state=42, stratify=df['family'])\n",
    "valid_df, test_df = train_test_split(test_valid_df, test_size=0.5, shuffle=True, random_state=42, stratify=test_valid_df['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Organize the data into train/val/test directories\n",
    "image_base_path = Path(\"../data/rare_species\")\n",
    "base_output_dir = image_base_path\n",
    "\n",
    "# # Copy the images to the new directories\n",
    "organize_split(image_base_path=image_base_path, base_output_dir=base_output_dir, split_df=train_df, split_name=\"train\")\n",
    "organize_split(image_base_path=image_base_path, base_output_dir=base_output_dir, split_df=valid_df, split_name=\"val\")\n",
    "organize_split(image_base_path=image_base_path, base_output_dir=base_output_dir, split_df=test_df, split_name=\"test\")\n",
    "\n",
    "#Clean up unnecessary folders\n",
    "cleanup_folders(image_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\rare_species\\\\mollusca_unionidae\\\\12853737_449393_eol-full-size-copy.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['full_file_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update the file paths to point to the new directories\n",
    "train_df = update_paths(train_df, \"train\")\n",
    "valid_df = update_paths(valid_df, \"val\")\n",
    "test_df = update_paths(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\rare_species\\\\train\\\\geoemydidae\\\\20969394_793083_eol-full-size-copy.jpg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['full_file_path'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export each DataFrame as a .pkl file\n",
    "#with open(\"../data/train_df.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(train_df, f)\n",
    "\n",
    "# with open(\"../data/valid_df.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(valid_df, f)\n",
    "\n",
    "#with open(\"../data/test_df.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(test_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sampling train\n",
    "# train_df_sampled, _ = train_test_split(train_df, test_size=0.5, shuffle=True, random_state=42, stratify=train_df['family'])\n",
    "# organize_split(image_base_path=image_base_path, base_output_dir=base_output_dir, split_df=train_df_sampled, split_name=\"train_sampled\")\n",
    "# train_df_sampled = update_paths(train_df_sampled, \"train_sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_counts = train_df['family'].value_counts().nlargest(100)\n",
    "# plot_graph(title='Distribution of Observations - Train Dataset', xlabel='Family', ylabel='Number of Observations',  counts=train_df_counts )\n",
    "\n",
    "# valid_df_counts = valid_df['family'].value_counts().nlargest(100)\n",
    "# plot_graph(title='Distribution of Observations - Validation Dataset', xlabel='Family', ylabel='Number of Observations',  counts=valid_df_counts )\n",
    "\n",
    "# test_df_counts = test_df['family'].value_counts().nlargest(100)\n",
    "# plot_graph(title='Distribution of Observations - Test Dataset', xlabel='Family', ylabel='Number of Observations',  counts=test_df_counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Training samples: {len(train_df)}\")\n",
    "# # print(f\"Training sampled samples: {len(train_df_sampled)}\")\n",
    "# print(f\"Validation samples: {len(valid_df)}\")\n",
    "# print(f\"Testing samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **5.** Preprocessing\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizes pixel values (e.g., rescaling from [0,255] to [0,1]).\n",
    "- Resizes images to a fixed size (e.g., 224x224 pixels).\n",
    "- Applies augmentation (only during training).\n",
    "- Converts images to batches (e.g., batch_size=32 loads 32 images at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrames from the .pkl files\n",
    "with open(\"../data/rare_species/train_df.pkl\", \"rb\") as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "with open(\"../data/rare_species/valid_df.pkl\", \"rb\") as f:\n",
    "    valid_df = pickle.load(f)\n",
    "\n",
    "with open(\"../data/rare_species/test_df.pkl\", \"rb\") as f:\n",
    "    test_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minority_class = train_df['family'].value_counts()[train_df['family'].value_counts() < 25].index\n",
    "# minority_class=minority_class.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32 ## the less the better because in each epoch the model sees N / batch_size images\n",
    "# image_size = (224, 224)\n",
    "\n",
    "# preprocess = Preprocessor(image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "# train_ds, class_names = preprocess.load_img(data_dir=\"../data/rare_species/train\", minority_class=minority_class, augment='mixup', oversampling=True)\n",
    "# train_df_sampled, class_names = preprocess.load_img(data_dir=\"../data/rare_species/train_sampled\", minority_class=minority_class, augment='mixup', oversampling=True)\n",
    "# val_ds, _ = preprocess.load_img(data_dir=\"../data/rare_species/val\", minority_class=minority_class, augment=None, oversampling=False)\n",
    "# test_ds, _ = preprocess.load_img(data_dir=\"../data/rare_species/tes\", minority_class=minority_class, augment=None, oversampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_images = 16 ##\n",
    "# rows, cols = 4, 4 ##\n",
    "\n",
    "# plot_batch(train_ds, class_names=class_names, num_images=num_images, rows=rows, cols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets for training, validation, and testing with both image and phylum as inputs\n",
    "# train_ds = build_ds_with_phylum(train_df)\n",
    "# val_ds = build_ds_with_phylum(valid_df)\n",
    "# test_ds = build_ds_with_phylum(test_df)\n",
    "\n",
    "\n",
    "# train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from glob import glob\n",
    "\n",
    "# train_dir = Path(\"../data/rare_species/train\")\n",
    "\n",
    "# image_paths = glob(os.path.join(train_dir, \"*\", \"*.jpg\"))  # ou .png dependendo do teu caso\n",
    "\n",
    "# print(f\"Total de imagens encontradas: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# # Modelo ResNet50 para extrair embeddings\n",
    "# model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# def extract_embedding(img_path):\n",
    "#     try:\n",
    "#         img = image.load_img(img_path, target_size=(224, 224))\n",
    "#         x = image.img_to_array(img)\n",
    "#         x = preprocess_input(np.expand_dims(x, axis=0))\n",
    "#         return model.predict(x, verbose=0)[0]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro com {img_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Extrair os embeddings\n",
    "# embeddings = []\n",
    "# valid_paths = []\n",
    "\n",
    "# for path in image_paths:\n",
    "#     emb = extract_embedding(path)\n",
    "#     if emb is not None:\n",
    "#         embeddings.append(emb)\n",
    "#         valid_paths.append(path)\n",
    "\n",
    "# embeddings = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Calcula a matriz de distâncias entre embeddings\n",
    "# dists = pairwise_distances(embeddings)\n",
    "\n",
    "# # Para cada ponto, encontra a distância ao seu 5º vizinho mais próximo\n",
    "# sorted_dists = np.sort(dists, axis=1)\n",
    "# k_distances = sorted_dists[:, 10]  # min_samples = 5\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.sort(k_distances))\n",
    "# plt.title(\"K-distance plot (5º vizinho)\")\n",
    "# plt.xlabel(\"Ponto\")\n",
    "# plt.ylabel(\"Distância ao 5º vizinho\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# import numpy as np\n",
    "\n",
    "# # DBSCAN: ajusta eps conforme o teu dataset\n",
    "# clustering = DBSCAN(eps=45, min_samples=10).fit(embeddings)\n",
    "# labels = clustering.labels_\n",
    "\n",
    "# # -1 indica outliers\n",
    "# outliers = np.where(labels == -1)[0]\n",
    "# outlier_paths = [valid_paths[i] for i in outliers]\n",
    "\n",
    "# print(f\"Número de outliers encontrados: {len(outlier_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(outlier_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# def show_images(image_paths, n=20, cols=5, title=\"Outliers\"):\n",
    "#     plt.figure(figsize=(15, (n // cols + 1) * 3))\n",
    "#     for i, img_path in enumerate(image_paths[:n]):\n",
    "#         img = mpimg.imread(img_path)\n",
    "#         plt.subplot(n // cols + 1, cols, i + 1)\n",
    "#         plt.imshow(img)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(os.path.basename(img_path), fontsize=8)\n",
    "#     plt.suptitle(title, fontsize=16)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# show_images(outlier_paths, n=100, cols=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations_to_test = [\n",
    "#     # \"none\",\n",
    "#     # \"light\",\n",
    "#     # \"medium\",\n",
    "#     # \"heavy\",\n",
    "#     # \"grayscale\",\n",
    "#     # \"randaugment\",\n",
    "#     \"mixup\",\n",
    "#     \"cutmix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ##### Simple model do test augmentations -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base = keras.applications.EfficientNetB0(\n",
    "#         input_shape=(224, 224, 3),\n",
    "#         include_top=False,\n",
    "#         weights=\"imagenet\",\n",
    "#         pooling=\"avg\"\n",
    "#     )\n",
    "#     base.trainable = False  # You can fine-tune later\n",
    "\n",
    "#     inputs = keras.Input(shape=(224, 224, 3))\n",
    "#     x = base(inputs, training=False)\n",
    "#     x = keras.layers.Dropout(0.2)(x)\n",
    "#     outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "#     return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_dataset(dataset, fraction=None, num_batches=None, seed=42):\n",
    "#     \"\"\"Return a sampled subset of the dataset.\"\"\"\n",
    "#     if fraction:\n",
    "#         dataset = dataset.shuffle(1000, seed=seed)\n",
    "#         dataset = dataset.take(int(fraction * tf.data.experimental.cardinality(dataset).numpy()))\n",
    "#     elif num_batches:\n",
    "#         dataset = dataset.take(num_batches)\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ##### Loop -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the preprocessor\n",
    "# pre = Preprocessor(image_size=(224, 224), batch_size=32)\n",
    "\n",
    "# # Store results\n",
    "# results = {}\n",
    "\n",
    "# # Loop through each augmentation\n",
    "# for aug in augmentations_to_test:\n",
    "#     print(f\"\\nTraining with augmentation: {aug}\")\n",
    "\n",
    "#     # Load datasets\n",
    "#     train_ds, class_names = pre.load_img(\n",
    "#         data_dir=\"../data/rare_species/train\",\n",
    "#         augment=aug\n",
    "#     )\n",
    "\n",
    "#     val_ds, _ = pre.load_img(\n",
    "#         data_dir=\"../data/rare_species/val\",\n",
    "#         augment=None\n",
    "#     )\n",
    "\n",
    "#     # Sample a subset of training data\n",
    "#     train_ds = sample_dataset(train_ds, fraction=0.5)\n",
    "\n",
    "#     # Build a fresh model (you should define this function)\n",
    "#     model = build_sequential_model(list_of_layers=layers)\n",
    "\n",
    "#     # Compile\n",
    "#     model.compile(\n",
    "#         optimizer=\"adam\",\n",
    "#         loss=\"categorical_crossentropy\",\n",
    "#         metrics=[\"accuracy\"]\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     history = model.fit(\n",
    "#         train_ds,\n",
    "#         validation_data=val_ds,\n",
    "#         epochs=5,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     # Predict entire validation set at once\n",
    "#     preds = model.predict(val_ds)\n",
    "#     y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "#     # Extract true labels in order\n",
    "#     y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for _, y in val_ds])\n",
    "\n",
    "#     # Compute metrics\n",
    "#     f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "#     f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
    "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "#     # Store in results\n",
    "#     results[aug] = {\n",
    "#         \"val_accuracy\": history.history[\"val_accuracy\"][-1],\n",
    "#         \"f1_macro\": f1_macro,\n",
    "#         \"f1_weighted\": f1_weighted,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall\n",
    "#     }\n",
    "\n",
    "#     print(f\"Finished '{aug}'\")\n",
    "#     print(f\"  Accuracy:      {results[aug]['val_accuracy']:.4f}\")\n",
    "#     print(f\"  F1 (macro):    {results[aug]['f1_macro']:.4f}\")\n",
    "#     print(f\"  F1 (weighted): {results[aug]['f1_weighted']:.4f}\")\n",
    "#     print(f\"  Precision:     {results[aug]['precision']:.4f}\")\n",
    "#     print(f\"  Recall:        {results[aug]['recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations_to_test = [\n",
    "#     \"none\",\n",
    "#     \"light\",\n",
    "#     \"medium\",\n",
    "#     \"heavy\",\n",
    "#     \"grayscale\",\n",
    "#     \"randaugment\",\n",
    "#     \"mixup\",\n",
    "#     \"cutmix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the preprocessor\n",
    "# pre = Preprocessor(image_size=(224, 224), batch_size=32)\n",
    "\n",
    "# # Store results\n",
    "# results = {}\n",
    "\n",
    "# # Loop through each augmentation\n",
    "# for aug in augmentations_to_test:\n",
    "#     print(f\"\\nTraining with augmentation: {aug}\")\n",
    "\n",
    "#     # Load datasets\n",
    "#     train_ds, class_names = pre.load_img(\n",
    "#         data_dir=\"../data/rare_species/train\",\n",
    "#         augment=aug\n",
    "#     )\n",
    "\n",
    "#     val_ds, _ = pre.load_img(\n",
    "#         data_dir=\"../data/rare_species/val\",\n",
    "#         augment=None\n",
    "#     )\n",
    "\n",
    "#     # Sample a subset of training data\n",
    "#     train_ds = sample_dataset(train_ds, fraction=0.5)\n",
    "\n",
    "#     # Build a fresh model (you should define this function)\n",
    "#     model = build_sequential_model(list_of_layers=layers)\n",
    "\n",
    "#     # Compile\n",
    "#     model.compile(\n",
    "#         optimizer=\"adam\",\n",
    "#         loss=\"categorical_crossentropy\",\n",
    "#         metrics=[\"accuracy\"]\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     history = model.fit(\n",
    "#         train_ds,\n",
    "#         validation_data=val_ds,\n",
    "#         epochs=15,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     # Predict entire validation set at once\n",
    "#     preds = model.predict(val_ds)\n",
    "#     y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "#     # Extract true labels in order\n",
    "#     y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for _, y in val_ds])\n",
    "\n",
    "#     # Compute metrics\n",
    "#     f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "#     f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
    "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "#     # Store in results\n",
    "#     results[aug] = {\n",
    "#         \"val_accuracy\": history.history[\"val_accuracy\"][-1],\n",
    "#         \"f1_macro\": f1_macro,\n",
    "#         \"f1_weighted\": f1_weighted,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall\n",
    "#     }\n",
    "\n",
    "#     print(f\"Finished '{aug}'\")\n",
    "#     print(f\"  Accuracy:      {results[aug]['val_accuracy']:.4f}\")\n",
    "#     print(f\"  F1 (macro):    {results[aug]['f1_macro']:.4f}\")\n",
    "#     print(f\"  F1 (weighted): {results[aug]['f1_weighted']:.4f}\")\n",
    "#     print(f\"  Precision:     {results[aug]['precision']:.4f}\")\n",
    "#     print(f\"  Recall:        {results[aug]['recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# **6.** Models\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom - Carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class = train_df['family'].value_counts()[train_df['family'].value_counts() < 25].index\n",
    "minority_class=minority_class.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minority_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/rare_species/train/unionidae/12853737_449393_eol-full-size-copy.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['full_file_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image is valid!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "path = '../data/rare_species/train/unionidae/12853737_449393_eol-full-size-copy.jpg'\n",
    "\n",
    "try:\n",
    "    img = Image.open(path)\n",
    "    img.verify()  # Check for corruption\n",
    "    print(\"Image is valid!\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, steps_per_epoch = build_ds_with_phylum_augmentation(train_df,minority_class=minority_class, oversampling=True, train=True)\n",
    "val_ds, _ = build_ds_with_phylum_augmentation(valid_df)\n",
    "test_ds, _ = build_ds_with_phylum_augmentation(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate\n",
    "\n",
    "# Inputs\n",
    "num_families=202\n",
    "image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "phylum_input = Input(shape=(5,), name=\"phylum_input\")\n",
    "\n",
    "# Image branch (a small CNN from scratch)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Concatenate with phylum input\n",
    "x = Concatenate()([x, phylum_input])\n",
    "\n",
    "# Dense layers\n",
    "x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Output layer for family classification\n",
    "output = Dense(num_families, activation='softmax')(x)  # set num_families accordingly\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[image_input, phylum_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"accuracy\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_macro\"),\n",
    "    F1Score(average=\"weighted\", name=\"f1_weighted\"),\n",
    "    TopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "]\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=loss_fn,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting from scratch.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m n_epochs=\u001b[32m20\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m history = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# callbacks=None use default callbacks (created in the Experiment class)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\2 SEMESTRE\\deep-learning-project\\project\\classes.py:437\u001b[39m, in \u001b[36mExperiment.run_experiment\u001b[39m\u001b[34m(self, epochs, callbacks, steps_per_epoch)\u001b[39m\n\u001b[32m    434\u001b[39m     all_callbacks = default_callbacks\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:410\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    405\u001b[39m     val_logs = {\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[33m\"\u001b[39m + name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    407\u001b[39m     }\n\u001b[32m    408\u001b[39m     epoch_logs.update(val_logs)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m training_logs = epoch_logs\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:145\u001b[39m, in \u001b[36mCallbackList.on_epoch_end\u001b[39m\u001b[34m(self, epoch, logs)\u001b[39m\n\u001b[32m    143\u001b[39m logs = python_utils.pythonify_logs(logs)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\2 SEMESTRE\\deep-learning-project\\project\\classes.py:361\u001b[39m, in \u001b[36mExperiment.ExperimentLogger.on_epoch_end\u001b[39m\u001b[34m(self, epoch, logs)\u001b[39m\n\u001b[32m    358\u001b[39m logs = logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# Compute F1 score for training and validation datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m f1_train_macro, f1_train_weighted = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m f1_val_macro, f1_val_weighted = \u001b[38;5;28mself\u001b[39m._compute_f1(\u001b[38;5;28mself\u001b[39m.val_ds)\n\u001b[32m    364\u001b[39m timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\2 SEMESTRE\\deep-learning-project\\project\\classes.py:347\u001b[39m, in \u001b[36mExperiment.ExperimentLogger._compute_f1\u001b[39m\u001b[34m(self, ds)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_f1\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     y_true = np.concatenate(\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    348\u001b[39m     y_pred_probs = \u001b[38;5;28mself\u001b[39m.model.predict(ds)\n\u001b[32m    349\u001b[39m     y_pred = np.argmax(y_pred_probs, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\2 SEMESTRE\\deep-learning-project\\project\\classes.py:347\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_f1\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     y_true = np.concatenate(\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    348\u001b[39m     y_pred_probs = \u001b[38;5;28mself\u001b[39m.model.predict(ds)\n\u001b[32m    349\u001b[39m     y_pred = np.argmax(y_pred_probs, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[39m, in \u001b[36mOwnedIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    825\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    827\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m errors.OutOfRangeError:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[39m, in \u001b[36mOwnedIterator._next_internal\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context.execution_mode(context.SYNC):\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m   ret = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    782\u001b[39m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._element_spec._from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Carolina\\Documents\\MESTRADO\\deep_learning\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3108\u001b[39m, in \u001b[36miterator_get_next\u001b[39m\u001b[34m(iterator, output_types, output_shapes, name)\u001b[39m\n\u001b[32m   3106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3107\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3108\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIteratorGetNext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_types\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_shapes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3112\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize the experiment\n",
    "experiment = Experiment(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    experiment_name=\"functional_model_phylum_grey_rotation\",\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "\n",
    "# Default callbacks\n",
    "# ExperimentLogger,  # Log experiment results after each epoch\n",
    "# EarlyStopping(patience=3, restore_best_weights=True),  # Early stopping callback\n",
    "# ModelCheckpoint(checkpoint_file, save_best_only=True)  # Save best model based on validation performance\n",
    "\n",
    "# Add callbacks\n",
    "callbacks =[\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "n_epochs=20\n",
    "# Run the experiment\n",
    "history = experiment.run_experiment(callbacks=callbacks, epochs=n_epochs, steps_per_epoch=steps_per_epoch) # callbacks=None use default callbacks (created in the Experiment class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Phylum (Marco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image input\n",
    "image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "base_model = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=image_input)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Phylum input\n",
    "phylum_input = Input(shape=(5,), name=\"phylum_input\")\n",
    "\n",
    "# Combine both\n",
    "combined = Concatenate()([x, phylum_input])\n",
    "combined = Dense(256, activation='relu')(combined)\n",
    "combined = Dropout(0.3)(combined)\n",
    "output = Dense(202, activation='softmax')(combined)\n",
    "\n",
    "model = Model(inputs=[image_input, phylum_input], outputs=output)\n",
    "\n",
    "# Define the metrics\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"accuracy\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_macro\"),\n",
    "    F1Score(average=\"weighted\", name=\"f1_weighted\"),\n",
    "    TopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TopKCategoricalAccuracy(k=5): https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy\n",
    "\n",
    "This is a more forgiving metric. It considers a prediction correct if the true label is among the model’s top 5 predictions (sorted by probability).\n",
    "Useful when there are many classes (like 202 families) — even if top-1 is hard, top-5 can still show useful trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the experiment\n",
    "experiment = Experiment(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    experiment_name=\"resnet50_with_phylum\",\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "\n",
    "# Default callbacks\n",
    "# ExperimentLogger,  # Log experiment results after each epoch\n",
    "# EarlyStopping(patience=3, restore_best_weights=True),  # Early stopping callback\n",
    "# ModelCheckpoint(checkpoint_file, save_best_only=True)  # Save best model based on validation performance\n",
    "\n",
    "# Add callbacks\n",
    "# callbacks = [\n",
    "\n",
    "# ]\n",
    "\n",
    "# Run the experiment\n",
    "history = experiment.run_experiment(callbacks=None, epochs=10) # callbacks=None use default callbacks (created in the Experiment class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Conv2D(16, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "          MaxPooling2D((2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
    "          Dense(202, activation=\"softmax\")\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sequential_model = build_sequential_model(list_of_layers=layers)\n",
    "first_sequential_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1 ##\n",
    "batch_size = 32 ##\n",
    "initial_lr = 0.01 ##\n",
    "final_lr = 0.001 ##\n",
    "\n",
    "verbose = 1\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"accuracy\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_score\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scheduler_fn = lr_scheduler(initial_lr, final_lr, n_epochs)\n",
    "callbacks = get_callbacks(\n",
    "    checkpoint_file_path=\"checkpoints/model.keras\",\n",
    "    metrics_file_path=\"logs/run1.csv\",\n",
    "    lr_scheduler=my_scheduler_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_sequential_model.compile(optimizer='rmsprop', loss= 'categorical_crossentropy', metrics= metrics)\n",
    "history = first_sequential_model.fit(train_ds, epochs= n_epochs, verbose= 1, batch_size = batch_size, validation_data= val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = history.history['accuracy']\n",
    "acc_val = history.history['val_accuracy']\n",
    "plot_model_acc(num_epochs=n_epochs, train_acc=acc_train, val_acc=acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['accuracy']\n",
    "loss_val = history.history['val_accuracy']\n",
    "plot_model_loss(num_epochs=n_epochs, train_loss=loss_train, val_loss=loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Model on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"acc\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_score\")\n",
    "]\n",
    "\n",
    "initial_lr = 1e-3  \n",
    "final_lr = 1e-4\n",
    "n_epochs = 50\n",
    "my_scheduler_fn = lr_scheduler(initial_lr, final_lr, n_epochs)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Junta ao teu callbacks list\n",
    "callbacks = get_callbacks(\n",
    "    checkpoint_file_path=\"checkpoints/model.keras\",\n",
    "    metrics_file_path=\"logs/run2.csv\",\n",
    "    lr_scheduler=my_scheduler_fn\n",
    ") + [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation=\"relu\", padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(202, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=input_img, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-3), loss='categorical_crossentropy', metrics=metrics)\n",
    "history = model.fit(train_ds, epochs=20, batch_size=20, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 - Margarida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"acc\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_score\")\n",
    "]\n",
    "\n",
    "initial_lr = 1e-4  \n",
    "final_lr = 1e-5\n",
    "n_epochs = 50\n",
    "my_scheduler_fn = lr_scheduler(initial_lr, final_lr, n_epochs)\n",
    "\n",
    "\n",
    "callbacks = get_callbacks(\n",
    "    checkpoint_file_path=\"checkpoints/model.keras\",\n",
    "    metrics_file_path=\"logs/run2.csv\",\n",
    "  lr_scheduler=my_scheduler_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_img)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(202, activation='softmax')(x)\n",
    "\n",
    "# Define final model\n",
    "model = models.Model(inputs=input_img, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=metrics)\n",
    "history = model.fit(train_ds, epochs=100, batch_size=16, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['accuracy']\n",
    "loss_val = history.history['val_accuracy']\n",
    "plot_model_acc(num_epochs=5, train_loss=loss_train, train_val=loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "xaxis = range(1,epochs+1)\n",
    "plt.plot(xaxis, loss_train, 'g', label='Training loss')\n",
    "plt.plot(xaxis, loss_val, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Model on Test Data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 - Margarida - com preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_ds, class_names = preprocess.load_img(\n",
    "    data_dir=\"../data/rare_species/train\",\n",
    "    minority_class=minority_class,\n",
    "    augment='mixup',\n",
    "    oversampling=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "val_ds, _ = preprocess.load_img(\n",
    "    data_dir=\"../data/rare_species/val\",\n",
    "    minority_class=minority_class,\n",
    "    augment=None,\n",
    "    preprocessing_function=preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"accuracy\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_score\")\n",
    "]\n",
    "\n",
    "initial_lr = 1e-4  \n",
    "final_lr = 1e-5\n",
    "n_epochs = 50\n",
    "my_scheduler_fn = lr_scheduler(initial_lr, final_lr, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(224, 224, 3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_img)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(202, activation='softmax', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "\n",
    "model = models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    experiment_name=\"resnet50_with_preprocessing_pre_finetuning\",\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "\n",
    "# Callbacks to reduce overfitting\n",
    "callbacks_additional = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,           # Reduce LR if no improvement after 2 epochs\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )]\n",
    "    \n",
    "# Run the experiment\n",
    "history = experiment.run_experiment(callbacks=callbacks_additional, epochs=7) # callbacks=None use default callbacks (created in the Experiment class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=7, batch_size=32, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == \"conv5_block1_1_conv\":\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=metrics)\n",
    "\n",
    "history = model.fit(train_ds, epochs=50, batch_size=32, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 1\n",
    "print(\"Fitting the end-to-end model\")\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"acc\"),\n",
    "    AUC(name=\"auc\"),\n",
    "    F1Score(average=\"macro\", name=\"f1_score\")\n",
    "]\n",
    "\n",
    "#initial_lr = 1e-4  \n",
    "#final_lr = 1e-5\n",
    "n_epochs = 50\n",
    "#my_scheduler_fn = lr_scheduler(initial_lr, final_lr, n_epochs)\n",
    "\n",
    "\n",
    "callbacks = get_callbacks(\n",
    "    checkpoint_file_path=\"checkpoints/model.keras\",\n",
    "    metrics_file_path=\"logs/run2.csv\",\n",
    "#   lr_scheduler=my_scheduler_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Número de classes (families) — substitui com o teu número real\n",
    "num_classes = len(df['family'].unique())\n",
    "\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "# congela só as primeiras camadas\n",
    "for layer in base_model.layers[:150]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Topo personalizado\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(202, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compilar\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"checkpoints/model.keras\", monitor='val_loss', save_best_only=True),\n",
    "    CSVLogger(\"logs/run2.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
