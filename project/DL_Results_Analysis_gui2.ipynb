{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02e283f2",
      "metadata": {
        "id": "02e283f2"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "# **1.** Environment Setup\n",
        "\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files"
      ],
      "metadata": {
        "id": "lH7_J4nRAjkk"
      },
      "id": "lH7_J4nRAjkk",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "W72TkTynAmnT"
      },
      "id": "W72TkTynAmnT",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if is_colab:\n",
        "    base_path = \"/content/drive/.shortcut-targets-by-id/1hNB4s6RR7JeKfFdGwm27WrF6pNXXIg3R/Deep Learning\"\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    os.chdir('/content/drive/MyDrive/Deep Learning')\n",
        "    print(\"Changed directory to:\", os.getcwd())\n",
        "    !pip install keras_cv\n",
        "    !pip install -q -U keras-tuner\n",
        "else:\n",
        "    base_path = \"..\""
      ],
      "metadata": {
        "id": "NnaPqtbRAox4",
        "outputId": "0805586d-32cc-4ebd-86ad-44e65987b0f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NnaPqtbRAox4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Changed directory to: /content/drive/.shortcut-targets-by-id/1hNB4s6RR7JeKfFdGwm27WrF6pNXXIg3R/Deep Learning\n",
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_cv) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras_cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras_cv) (4.9.8)\n",
            "Collecting keras-core (from keras_cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras_cv) (0.3.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (3.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.7.1)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (5.29.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.17.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (3.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.13.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2025.1.31)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras_cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (2.19.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->keras_cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.70.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras_cv\n",
            "Successfully installed keras-core-0.1.7 keras_cv-0.9.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d815d3b",
      "metadata": {
        "id": "3d815d3b"
      },
      "source": [
        "## 1.1 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "89ef556b",
      "metadata": {
        "id": "89ef556b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from classes import *\n",
        "from functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "55268a9a",
      "metadata": {
        "id": "55268a9a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Concatenate, Dropout, Input, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
        "from keras.metrics import AUC, F1Score, CategoricalAccuracy, TopKCategoricalAccuracy\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the DataFrames from the .pkl files\n",
        "with open(os.path.join(base_path,\"data/train_df.pkl\"), \"rb\") as f:\n",
        "     train_df = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_path,\"data/valid_df.pkl\"), \"rb\") as f:\n",
        "     val_df = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_path,\"data/test_df.pkl\"), \"rb\") as f:\n",
        "     test_df = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_path,\"data/train_df_sampled.pkl\"), \"rb\") as f:\n",
        "     train_df_sampled = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_path,\"data/family_encoder.pkl\"), \"rb\") as f:\n",
        "     family_encoder = pickle.load(f)"
      ],
      "metadata": {
        "id": "0hMHy4gsBGa-"
      },
      "id": "0hMHy4gsBGa-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = family_encoder.classes_\n",
        "\n",
        "class_names"
      ],
      "metadata": {
        "id": "TyiaZbJsBvxM",
        "outputId": "4bb0f9bb-c342-439d-be25-d6ec518a219d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TyiaZbJsBvxM",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['accipitridae', 'acipenseridae', 'acroporidae', 'agamidae',\n",
              "       'agariciidae', 'albulidae', 'alcedinidae', 'alligatoridae',\n",
              "       'alopiidae', 'ambystomatidae', 'anatidae', 'anguidae', 'aotidae',\n",
              "       'apidae', 'ardeidae', 'arthroleptidae', 'atelidae', 'attelabidae',\n",
              "       'balaenicipitidae', 'balaenidae', 'balaenopteridae', 'balistidae',\n",
              "       'bombycillidae', 'bovidae', 'brachypteraciidae', 'bucerotidae',\n",
              "       'bufonidae', 'burhinidae', 'cacatuidae', 'callitrichidae',\n",
              "       'callorhinchidae', 'caprimulgidae', 'carabidae', 'carcharhinidae',\n",
              "       'cardiidae', 'carettochelyidae', 'cebidae', 'cerambycidae',\n",
              "       'cercopithecidae', 'cervidae', 'cetorhinidae', 'chaetodontidae',\n",
              "       'chamaeleonidae', 'charadriidae', 'cheirogaleidae', 'chelidae',\n",
              "       'cheloniidae', 'chelydridae', 'ciconiidae', 'coenagrionidae',\n",
              "       'colubridae', 'columbidae', 'conidae', 'cracidae', 'cricetidae',\n",
              "       'crocodylidae', 'cryptobranchidae', 'ctenomyidae', 'cuculidae',\n",
              "       'cyprinodontidae', 'dactyloidae', 'dalatiidae', 'dasyatidae',\n",
              "       'dasypodidae', 'dasyuridae', 'daubentoniidae', 'delphinidae',\n",
              "       'dendrobatidae', 'dendrophylliidae', 'diomedeidae',\n",
              "       'diploastraeidae', 'diplodactylidae', 'elapidae', 'emydidae',\n",
              "       'equidae', 'estrildidae', 'euphylliidae', 'falconidae', 'faviidae',\n",
              "       'formicidae', 'fringillidae', 'fungiidae', 'gavialidae',\n",
              "       'gekkonidae', 'geoemydidae', 'giraffidae', 'glareolidae',\n",
              "       'gliridae', 'gomphidae', 'goodeidae', 'gymnuridae', 'haliotidae',\n",
              "       'helioporidae', 'hemiscylliidae', 'hexanchidae', 'hominidae',\n",
              "       'hyaenidae', 'hylobatidae', 'hynobiidae', 'iguanidae', 'indriidae',\n",
              "       'labridae', 'lacertidae', 'lamnidae', 'laridae', 'latimeriidae',\n",
              "       'lemuridae', 'leporidae', 'lobophylliidae', 'lucanidae',\n",
              "       'lutjanidae', 'manidae', 'mantellidae', 'meandrinidae',\n",
              "       'megapodiidae', 'merlucciidae', 'merulinidae', 'mesitornithidae',\n",
              "       'mimidae', 'motacillidae', 'muscicapidae', 'mustelidae',\n",
              "       'myliobatidae', 'nesospingidae', 'nymphalidae', 'odontophoridae',\n",
              "       'otariidae', 'otididae', 'palinuridae', 'pangasiidae',\n",
              "       'papilionidae', 'paradisaeidae', 'pardalotidae', 'parulidae',\n",
              "       'percidae', 'phasianidae', 'phrynosomatidae', 'phyllomedusidae',\n",
              "       'phyllostomidae', 'pisauridae', 'pittidae', 'platystictidae',\n",
              "       'plethodontidae', 'pleuronectidae', 'pocilloporidae',\n",
              "       'podocnemididae', 'polyprionidae', 'pontoporiidae', 'potoroidae',\n",
              "       'pristidae', 'procellariidae', 'pseudophasmatidae', 'psittacidae',\n",
              "       'psittaculidae', 'pythonidae', 'rajidae', 'rallidae',\n",
              "       'ramphastidae', 'ranidae', 'recurvirostridae', 'rhacophoridae',\n",
              "       'rhinodermatidae', 'rhyacotritonidae', 'salamandridae',\n",
              "       'salmonidae', 'scincidae', 'sciuridae', 'scolopacidae',\n",
              "       'scombridae', 'serranidae', 'siderastreidae', 'siluridae',\n",
              "       'somniosidae', 'soricidae', 'sparidae', 'spheniscidae',\n",
              "       'sphyrnidae', 'squalidae', 'squatinidae', 'stichopodidae',\n",
              "       'strigidae', 'strigopidae', 'syngnathidae', 'testudinidae',\n",
              "       'tettigoniidae', 'theraphosidae', 'thraupidae', 'trionychidae',\n",
              "       'triopsidae', 'trochilidae', 'trogonidae', 'tropiduridae',\n",
              "       'turdidae', 'unionidae', 'urolophidae', 'ursidae', 'vangidae',\n",
              "       'vespertilionidae', 'viperidae', 'vireonidae', 'vombatidae',\n",
              "       'zonitidae'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803176e3",
      "metadata": {
        "id": "803176e3"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "# **2.** Preprocessing\n",
        "\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab5ad08",
      "metadata": {
        "id": "4ab5ad08"
      },
      "outputs": [],
      "source": [
        "with open(\"../data/train_df.pkl\", \"rb\") as f:\n",
        "     train_df = pickle.load(f)\n",
        "\n",
        "with open(\"../data/val_df.pkl\", \"rb\") as f:\n",
        "     val_df = pickle.load(f)\n",
        "\n",
        "with open(\"../data/test_df.pkl\", \"rb\") as f:\n",
        "     test_df = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc100d22",
      "metadata": {
        "id": "bc100d22"
      },
      "outputs": [],
      "source": [
        "with open('family_encoder.pkl', 'rb') as f:\n",
        "    family_encoder = pickle.load(f)\n",
        "\n",
        "# Load class names\n",
        "class_names = family_encoder.classes_\n",
        "\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a7ca1ca0",
      "metadata": {
        "id": "a7ca1ca0"
      },
      "outputs": [],
      "source": [
        "minority_class = train_df['family'].value_counts()[train_df['family'].value_counts() < 25].index\n",
        "minority_class=minority_class.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9020e1ec",
      "metadata": {
        "id": "9020e1ec"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 ## the less the better because in each epoch the model sees N / batch_size images\n",
        "image_size = (224, 224)\n",
        "\n",
        "preprocess = Preprocessor(image_size=image_size, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bae18c",
      "metadata": {
        "id": "58bae18c"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "# **3.** Results Analysis\n",
        "\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c51bd5d0",
      "metadata": {
        "id": "c51bd5d0",
        "outputId": "066ebd59-ee71-40ad-cda6-4392bbce2490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 34 variables whereas the saved optimizer has 6 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "# Load model keras\n",
        "model = load_model(os.path.join(base_path,\"models/efficient_net_finetuned_final.keras\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9376929",
      "metadata": {
        "id": "b9376929",
        "outputId": "07bdce52-d4f1-46b9-b40f-7b8d35cadd54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8388 files belonging to 202 classes.\n",
            "Found 1797 files belonging to 202 classes.\n",
            "Found 1798 files belonging to 202 classes.\n"
          ]
        }
      ],
      "source": [
        "# load datasets with efficientnets preprocessing\n",
        "train_ds, _ = preprocess.load_img(\n",
        "    data_dir=os.path.join(base_path,\"data/rare_species/train\"),\n",
        "    minority_class=minority_class,\n",
        "    augment=\"mixup\",\n",
        "    oversampling=True,\n",
        "    shuffle= True,\n",
        "    preprocessing_function=preprocess_input)\n",
        "\n",
        "val_ds, _ = preprocess.load_img(\n",
        "    data_dir=os.path.join(base_path,\"data/rare_species/val\"),\n",
        "    minority_class=[],\n",
        "    augment=None,\n",
        "    shuffle= False,\n",
        "    preprocessing_function=preprocess_input)\n",
        "\n",
        "test_ds, _ = preprocess.load_img(\n",
        "    data_dir=os.path.join(base_path,\"data/rare_species/test\"),\n",
        "    minority_class=[],\n",
        "    augment=None,\n",
        "    shuffle= False,\n",
        "    preprocessing_function=preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9d22e293",
      "metadata": {
        "id": "9d22e293",
        "outputId": "becb6f93-7895-4ce4-efe7-63166aa7a730",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 7s/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaecb56d",
      "metadata": {
        "id": "eaecb56d"
      },
      "source": [
        "### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metric(dataset, model_name):\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    model = load_model(model_name)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch_x, batch_y in dataset:\n",
        "        preds = model.predict(batch_x, verbose=0)\n",
        "        y_true.append(np.argmax(batch_y.numpy(), axis=1))\n",
        "        y_pred.append(np.argmax(preds, axis=1))\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "    print(\"Accuracy     :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"F1 (macro)   :\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(\"Precision    :\", precision_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(\"Recall       :\", recall_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "    return y_true, y_pred"
      ],
      "metadata": {
        "id": "p9pCacySGTrF"
      },
      "id": "p9pCacySGTrF",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7f0f72",
      "metadata": {
        "id": "ce7f0f72",
        "outputId": "794fa32d-e76a-4503-d7d1-61819e74b387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model: /content/drive/.shortcut-targets-by-id/1hNB4s6RR7JeKfFdGwm27WrF6pNXXIg3R/Deep Learning/models/efficient_net_finetuned_final.keras\n"
          ]
        }
      ],
      "source": [
        "get_metric(train_ds, os.path.join(base_path,\"models/efficient_net_finetuned_final.keras\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7452f6",
      "metadata": {
        "id": "1a7452f6"
      },
      "outputs": [],
      "source": [
        "get_metric(val_ds, os.path.join(base_path,\"models/efficient_net_finetuned_final.keras\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f844d1fa",
      "metadata": {
        "id": "f844d1fa"
      },
      "source": [
        "### Visualize no confidence images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "94972250",
      "metadata": {
        "id": "94972250",
        "outputId": "0dcc1788-8ba6-44cf-f4b5-345e9a03805d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3c7fd60f8cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ],
      "source": [
        "# Unbatch train_ds into list\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for img, label in train_ds.unbatch():\n",
        "    train_images.append(img.numpy())\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Predict all train images\n",
        "pred_probs_all = model.predict(train_images, verbose=1)\n",
        "y_pred = np.argmax(pred_probs_all, axis=1)\n",
        "y_true = train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76289de4",
      "metadata": {
        "id": "76289de4"
      },
      "outputs": [],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aff2142",
      "metadata": {
        "id": "2aff2142"
      },
      "outputs": [],
      "source": [
        "train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8843f90b",
      "metadata": {
        "id": "8843f90b"
      },
      "outputs": [],
      "source": [
        "pred_probs_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319eaf6d",
      "metadata": {
        "id": "319eaf6d"
      },
      "outputs": [],
      "source": [
        "np.max(pred_probs_all, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac587bd",
      "metadata": {
        "id": "6ac587bd"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of certainties\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(np.max(pred_probs_all, axis=1), bins=30, edgecolor='black')\n",
        "plt.xlabel('Model Certainty (Max Softmax Probability)')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Distribution of Model Certainty')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dade6ad",
      "metadata": {
        "id": "8dade6ad"
      },
      "source": [
        "#### Histogram of model confidence, divided by correctly and incorrectly classified images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb232c3c",
      "metadata": {
        "id": "cb232c3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- assume pred_probs_all and train_labels are already defined ---\n",
        "\n",
        "# 1) prepare labels\n",
        "labels = train_labels\n",
        "if labels.ndim > 1:\n",
        "    labels = np.argmax(labels, axis=1)\n",
        "\n",
        "# 2) get preds and confidences\n",
        "preds = np.argmax(pred_probs_all, axis=1)\n",
        "confidences = np.max(pred_probs_all, axis=1)\n",
        "\n",
        "# 3) split confidences by correctness\n",
        "correct_conf = confidences[preds == labels]\n",
        "incorrect_conf = confidences[preds != labels]\n",
        "\n",
        "# 4) define bins\n",
        "bins = np.linspace(0, 1, 21)  # 20 bins\n",
        "\n",
        "# 5) histogram counts for each bin\n",
        "correct_counts, _   = np.histogram(correct_conf,   bins=bins)\n",
        "incorrect_counts, _ = np.histogram(incorrect_conf, bins=bins)\n",
        "total_counts = correct_counts + incorrect_counts\n",
        "\n",
        "# 6) misclassification rate per bin (in %), guard against zero-division\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    misclass_rate = 100 * incorrect_counts / total_counts\n",
        "misclass_rate = np.nan_to_num(misclass_rate)  # zero where total_counts==0\n",
        "\n",
        "# 7) bin centers for plotting the line\n",
        "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "# 8) plot\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# stacked histogram\n",
        "ax1.hist(\n",
        "    [correct_conf, incorrect_conf],\n",
        "    bins=bins,\n",
        "    stacked=True,\n",
        "    color=['green', 'red'],\n",
        "    label=['Correct classification', 'Incorrect classification'],\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "ax1.set_xlabel('Model Confidence (Max Softmax Probability)')\n",
        "ax1.set_ylabel('Number of Samples')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# secondary axis for misclassification %\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(\n",
        "    bin_centers,\n",
        "    misclass_rate,\n",
        "    marker='o',\n",
        "    linestyle='-',\n",
        "    color='blue',\n",
        "    label='Misclassification rate (%)'\n",
        ")\n",
        "ax2.set_ylabel('Misclassification Rate (%)')\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.title('Distribution of Model Confidence')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d20e6c8",
      "metadata": {
        "id": "1d20e6c8"
      },
      "source": [
        "#### Trying to incorporate phylum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51884f0",
      "metadata": {
        "id": "e51884f0"
      },
      "outputs": [],
      "source": [
        "# Unbatch train_ds into list\n",
        "val_images = []\n",
        "val_labels = []\n",
        "\n",
        "for img, label in val_ds.unbatch():\n",
        "    val_images.append(img.numpy())\n",
        "    val_labels.append(label.numpy())\n",
        "\n",
        "val_images = np.array(val_images)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "# Predict all val images\n",
        "pred_probs_all_val = model.predict(val_images, verbose=1)\n",
        "y_pred = np.argmax(pred_probs_all, axis=1)\n",
        "y_true = val_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3182fc5e",
      "metadata": {
        "id": "3182fc5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create family:phylum dicionary\n",
        "meta = pd.read_csv('../data/rare_species/metadata.csv')\n",
        "dup = meta.groupby('family')['phylum'].nunique()\n",
        "family_to_phylum = dict(zip(meta['family'], meta['phylum']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c182f96",
      "metadata": {
        "id": "5c182f96"
      },
      "outputs": [],
      "source": [
        "# --- 2) Prepare true labels & names ---\n",
        "# train_labels: your true labels, shape (11200,) or one-hot (11200,202)\n",
        "# class_names: list of length 202 mapping index→family string\n",
        "labels = np.argmax(val_labels, axis=1) # family# prediction, not one-hot-encoded\n",
        "y_true_fam   = np.array([class_names[i] for i in labels])\n",
        "y_true_phyl  = np.array([family_to_phylum[f] for f in y_true_fam])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5238f7c4",
      "metadata": {
        "id": "5238f7c4"
      },
      "outputs": [],
      "source": [
        "# --- 3) Baseline predictions & phylum check ---\n",
        "y_pred       = np.argmax(pred_probs_all_val, axis=1)\n",
        "y_pred_fam   = np.array([class_names[i] for i in y_pred])\n",
        "y_pred_phyl  = np.array([family_to_phylum[f] for f in y_pred_fam])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c97d0f",
      "metadata": {
        "id": "49c97d0f"
      },
      "outputs": [],
      "source": [
        "# a dictionary phylum:[list of indices of classes that belong to that phylum]\n",
        "phylum_to_inds = {}\n",
        "for idx, fam in enumerate(class_names):\n",
        "    ph = family_to_phylum[fam]\n",
        "    phylum_to_inds.setdefault(ph, []).append(idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a924cea4",
      "metadata": {
        "id": "a924cea4"
      },
      "outputs": [],
      "source": [
        "# --- 4) Hierarchical override logic ---\n",
        "y_pred_hier = y_pred.copy()\n",
        "for i, probs in enumerate(pred_probs_all_val):\n",
        "    # if predicted phylum ≠ true phylum, force to best within true phylum\n",
        "    if y_pred_phyl[i] != y_true_phyl[i]:\n",
        "        valid_idx = phylum_to_inds[y_true_phyl[i]]\n",
        "        y_pred_hier[i] = valid_idx[np.argmax(probs[valid_idx])]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6535eee7",
      "metadata": {
        "id": "6535eee7"
      },
      "outputs": [],
      "source": [
        "# --- 5) Overall accuracy comparison ---\n",
        "acc_base  = accuracy_score(labels, y_pred)\n",
        "acc_hier  = accuracy_score(labels, y_pred_hier)\n",
        "print(f\"Baseline overall accuracy:     {acc_base:.4f}\")\n",
        "print(f\"Hierarchical override accuracy:{acc_hier:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd706552",
      "metadata": {
        "id": "fd706552"
      },
      "outputs": [],
      "source": [
        "print(\"Classification report with original predictions:\")\n",
        "print(classification_report(labels,y_pred))\n",
        "print(\"Classification report with original predictions:\")\n",
        "print(classification_report(labels,y_pred_hier,digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209f9b80",
      "metadata": {
        "id": "209f9b80"
      },
      "outputs": [],
      "source": [
        "# --- 6) Per-phylum accuracy improvement ---\n",
        "phyla = np.unique(y_true_phyl)\n",
        "base_scores = []\n",
        "hier_scores = []\n",
        "for ph in phyla:\n",
        "    idxs = np.where(y_true_phyl == ph)[0]\n",
        "    base_scores.append( accuracy_score(labels[idxs], y_pred[idxs]) )\n",
        "    hier_scores.append( accuracy_score(labels[idxs], y_pred_hier[idxs]) )\n",
        "\n",
        "x = np.arange(len(phyla))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x - width/2, base_scores, width, label='Baseline')\n",
        "plt.bar(x + width/2, hier_scores, width, label='Hierarchical')\n",
        "plt.xticks(x, phyla, rotation=45, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy by Phylum')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15927509",
      "metadata": {
        "id": "15927509"
      },
      "source": [
        "#### No certainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8003e3ed",
      "metadata": {
        "id": "8003e3ed"
      },
      "outputs": [],
      "source": [
        "# Convert y_true to class labels\n",
        "y_true_labels = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Get misclassified indices\n",
        "misclassified_indices = np.where(y_true_labels != y_pred)[0]\n",
        "\n",
        "# For each misclassified, get model confidence (highest softmax probability)\n",
        "confidences = np.max(pred_probs_all[misclassified_indices], axis=1)\n",
        "\n",
        "# Sort misclassified examples by descending confidence\n",
        "sorted_indices = np.argsort(confidences)  # Sort ascending\n",
        "selected_indices = misclassified_indices[sorted_indices[:12]]  # Take top 6\n",
        "\n",
        "# Prepare images to show (selected misclassified ones)\n",
        "images_to_show = []\n",
        "\n",
        "for idx in selected_indices:\n",
        "    img = train_images[idx]\n",
        "    true_label = y_true_labels[idx]\n",
        "    pred_label = y_pred[idx]\n",
        "    confidence = confidences[np.where(misclassified_indices == idx)][0]  # Get confidence for the current misclassified image\n",
        "    images_to_show.append((img, true_label, pred_label, confidence))\n",
        "\n",
        "# Save the misclassified data\n",
        "misclassified_data = {\n",
        "    \"True Label\": [class_names[true_label] for _, true_label, _, _ in images_to_show],\n",
        "    \"Predicted Label\": [class_names[pred_label] for _, _, pred_label, _ in images_to_show],\n",
        "    \"Confidence\": [confidence for _, _, _, confidence in images_to_show]\n",
        "}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, (img, true_label, pred_label, confidence) in enumerate(images_to_show):\n",
        "    plt.subplot(4, 3, i + 1)\n",
        "    plt.imshow(img.astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {confidence:.2f}\",\n",
        "              color='red', fontsize=10)\n",
        "\n",
        "plt.suptitle(\"Less Confident Misclassifications\", fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e726e3df",
      "metadata": {
        "id": "e726e3df"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of certainties\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(confidences, bins=30, edgecolor='black')\n",
        "plt.xlabel('Model Certainty (Max Softmax Probability)')\n",
        "plt.ylabel('Number of Misclassified Samples')\n",
        "plt.title('Distribution of Model Certainty for Misclassified Samples')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5dcfddc",
      "metadata": {
        "id": "f5dcfddc"
      },
      "source": [
        "Since you have 202 classes, a model that is completely uncertain (i.e., guessing randomly) would have maximum softmax probability 0.004. But models aren't truly random even when uncertain — usually, even bad predictions are around 0.02–0.1 certainty (depends on how the softmax behaves)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f75238",
      "metadata": {
        "id": "04f75238"
      },
      "outputs": [],
      "source": [
        "# First, compute the maximum probability for ALL predictions\n",
        "all_confidences = np.max(pred_probs_all, axis=1)  # Shape: (num_val_samples,)\n",
        "\n",
        "# Total number of validation images\n",
        "total_val_images = len(train_images)\n",
        "\n",
        "# Number of predictions where certainty < 0.1\n",
        "num_uncertain_predictions = np.sum(all_confidences < 0.019)\n",
        "\n",
        "print(f\"Total validation images: {total_val_images}\")\n",
        "print(f\"Number of predictions with certainty < 0.1: {num_uncertain_predictions}\")\n",
        "print(f\"Percentage: {100 * num_uncertain_predictions / total_val_images:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c8f26f",
      "metadata": {
        "id": "c9c8f26f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab43070",
      "metadata": {
        "id": "5ab43070"
      },
      "outputs": [],
      "source": [
        "misclassified_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc372ac",
      "metadata": {
        "id": "cdc372ac"
      },
      "source": [
        "#### Top Correctly Classified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d425d515",
      "metadata": {
        "id": "d425d515"
      },
      "outputs": [],
      "source": [
        "# Convert y_true to class labels\n",
        "y_true_labels = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Get indices of correctly classified images\n",
        "correct_indices = np.where(y_true_labels == y_pred)[0]\n",
        "\n",
        "# For each correctly classified image, get model confidence (highest softmax probability)\n",
        "confidences = np.max(pred_probs_all[correct_indices], axis=1)\n",
        "\n",
        "# Sort correctly classified examples by descending confidence\n",
        "sorted_indices = np.argsort(confidences)[::-1]  # Sort descending\n",
        "selected_indices = correct_indices[sorted_indices[:6]]  # Take top 6 with highest confidence\n",
        "\n",
        "# Prepare images to show (selected correctly classified ones)\n",
        "images_to_show = []\n",
        "\n",
        "for idx in selected_indices:\n",
        "    img = test_images[idx]\n",
        "    true_label = y_true_labels[idx]\n",
        "    pred_label = y_pred[idx]\n",
        "    confidence = confidences[np.where(correct_indices == idx)][0]  # Get confidence for the current correctly classified image\n",
        "    images_to_show.append((img, true_label, pred_label, confidence))\n",
        "\n",
        "# Save the misclassified data\n",
        "correctly_classified_data = {\n",
        "    \"True Label\": [class_names[true_label] for _, true_label, _, _ in images_to_show],\n",
        "    \"Predicted Label\": [class_names[pred_label] for _, _, pred_label, _ in images_to_show],\n",
        "    \"Confidence\": [confidence for _, _, _, confidence in images_to_show]\n",
        "}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, (img, true_label, pred_label, confidence) in enumerate(images_to_show):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.imshow(img.astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {confidence:.2f}\",\n",
        "              color='green', fontsize=10)\n",
        "\n",
        "plt.suptitle(\"Most Confident Correct Classifications\", fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011c201a",
      "metadata": {
        "id": "011c201a"
      },
      "outputs": [],
      "source": [
        "correctly_classified_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16268a15",
      "metadata": {
        "id": "16268a15"
      },
      "source": [
        "#### Correctly Classified for Specific Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc6e41a",
      "metadata": {
        "id": "bfc6e41a"
      },
      "outputs": [],
      "source": [
        "misclassified_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e9b4de",
      "metadata": {
        "id": "c8e9b4de"
      },
      "outputs": [],
      "source": [
        "for family in misclassified_data[\"True Label\"]:\n",
        "    show_correct_predictions_for_family(y_true, y_pred, pred_probs_all, test_images, class_names, family_name=family, num_images=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a197b6b7",
      "metadata": {
        "id": "a197b6b7"
      },
      "outputs": [],
      "source": [
        "for family in misclassified_data[\"Predicted Label\"]:\n",
        "    show_correct_predictions_for_family(y_true, y_pred, pred_probs_all, test_images, class_names, family_name=family, num_images=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87e2ac7",
      "metadata": {
        "id": "c87e2ac7"
      },
      "outputs": [],
      "source": [
        "correctly_classified_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e597f2aa",
      "metadata": {
        "id": "e597f2aa"
      },
      "outputs": [],
      "source": [
        "for family in correctly_classified_data[\"True Label\"]:\n",
        "    show_correct_predictions_for_family(y_true, y_pred, pred_probs_all, test_images, class_names, family_name=family, num_images=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2841d5",
      "metadata": {
        "id": "ee2841d5"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "# **4.** More\n",
        "\n",
        "<div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6c047a",
      "metadata": {
        "id": "5e6c047a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d6716f",
      "metadata": {
        "id": "31d6716f"
      },
      "outputs": [],
      "source": [
        "files = {\n",
        "    \"VGG16\": \"vgg_history_train_until_opt.csv\",\n",
        "    \"ResNet50\": \"resnet50_final_train_history.csv\",\n",
        "    \"EfficientNet Baseline 1\": \"efficient_net_baseline1_history.csv\",\n",
        "    \"EfficientNet Baseline 2\": \"efficient_net_baseline2_history.csv\",\n",
        "    \"EfficientNet Final\": \"efficient_net_final_train_history.csv\",\n",
        "    \"EfficientNet Fine-Tuned\": \"efficient_net_final_finetune_history.csv\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712f2994",
      "metadata": {
        "id": "712f2994"
      },
      "outputs": [],
      "source": [
        "def load_history(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if 'acc' in df.columns:\n",
        "        df = df.rename(columns={'acc': 'accuracy'})\n",
        "    if 'val_acc' in df.columns:\n",
        "        df = df.rename(columns={'val_acc': 'val_accuracy'})\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c69abdf",
      "metadata": {
        "id": "6c69abdf"
      },
      "outputs": [],
      "source": [
        "histories = {label: load_history(path) for label, path in files.items()}\n",
        "\n",
        "summary_list = []\n",
        "for label, df in histories.items():\n",
        "    last_epoch = df.iloc[-1]\n",
        "    # Para cada métrica, tenta pegar 'accuracy' e 'f1_score'\n",
        "    summary_list.append({\n",
        "        \"Model\": label,\n",
        "        \"Train Set Accuracy\": last_epoch.get(\"accuracy\", last_epoch.get(\"accuracy\", None)),\n",
        "        \"Validation Set Accuracy\": last_epoch.get(\"val_accuracy\", last_epoch.get(\"val_accuracy\", None)),\n",
        "        \"Train Set F1-Score\": last_epoch.get(\"f1_score\", last_epoch.get(\"f1_score\", None)),\n",
        "        \"Validation Set F1-Score\": last_epoch.get(\"val_f1_score\", last_epoch.get(\"val_f1_score\", None)),\n",
        "        \"Train Set Loss\": last_epoch.get(\"loss\", last_epoch.get(\"loss\", None)),\n",
        "        \"Validation Set Loss\": last_epoch.get(\"val_loss\", last_epoch.get(\"val_loss\", None))\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_list)\n",
        "summary_df = summary_df.set_index(\"Model\")\n",
        "summary_df = summary_df.sort_values(by=\"Validation Set F1-Score\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02ad7bf",
      "metadata": {
        "id": "a02ad7bf"
      },
      "source": [
        "### Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e482b87d",
      "metadata": {
        "id": "e482b87d"
      },
      "outputs": [],
      "source": [
        "summary_df = summary_df.sort_values(by=\"Validation Set F1-Score\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a745392",
      "metadata": {
        "id": "1a745392"
      },
      "outputs": [],
      "source": [
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de018a27",
      "metadata": {
        "id": "de018a27"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fd2996",
      "metadata": {
        "id": "16fd2996"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['font.family'] = 'Aptos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1606c6",
      "metadata": {
        "id": "1c1606c6"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(8, 10))\n",
        "ax = summary_df[['Train Set Accuracy', 'Validation Set Accuracy']].plot(kind='bar', color=['navy', 'maroon'], legend=False)\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9753727b",
      "metadata": {
        "id": "9753727b"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(8, 10))\n",
        "ax = summary_df[['Train Set Loss', 'Validation Set Loss']].plot(kind='bar', color=['navy', 'maroon'], legend=False)\n",
        "ax.legend(loc='upper left', fontsize=10)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim(0, 2.5)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830c197f",
      "metadata": {
        "id": "830c197f"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(8, 10))\n",
        "ax = summary_df[['Train Set F1-Score', 'Validation Set F1-Score']].plot(kind='bar', color=['navy', 'maroon'], legend=False)\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5416e338",
      "metadata": {
        "id": "5416e338"
      },
      "source": [
        "### Test Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d80618",
      "metadata": {
        "id": "13d80618"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a8c97c",
      "metadata": {
        "id": "25a8c97c"
      },
      "outputs": [],
      "source": [
        "test_ds, _ = preprocess.load_img(\n",
        "    data_dir=\"../data/rare_species/test\",\n",
        "    minority_class=[],\n",
        "    augment=None,\n",
        "    shuffle= False,\n",
        "    preprocessing_function=preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc649597",
      "metadata": {
        "id": "fc649597"
      },
      "outputs": [],
      "source": [
        "get_metric(test_ds, \"efficient_net_finetuned_final.keras\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}